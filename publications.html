<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Publications - Xu Li</title>
    <link rel="stylesheet" href="minimal-styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
</head>
<body>
    <header>
        <nav>
            <ul class="nav-links">
                <li><a href="index.html">home</a></li>
                <li><a href="research.html">research</a></li>
                <li><a href="publications.html">publications</a></li>
                <li><a href="cv.html">cv</a></li>
            </ul>
            <div class="theme-toggle">
                <i class="fas fa-moon"></i>
            </div>
        </nav>
    </header>

    <main>
        <section class="publications">
            <div class="container">
                <h1>Publications ÊùéÊó≠</h1>
                
                <div class="research-content">
                    <h2>2025</h2>
                    
                    <p><strong>üåü MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering</strong><br>
                    <em>Xu Li, Fan Lyu</em><br>
                    arXiv preprint (arXiv:2505.19455), May 2025<br>
                    <a href="https://arxiv.org/abs/2505.19455" target="_blank">paper</a> | <a href="#" target="_blank">code_fig1</a><br>
                    <br>
                    <strong>Abstract:</strong> Continual Visual Question Answering (CVQA) based on pre-trained models (PTMs) has achieved promising progress by leveraging prompt tuning to enable continual multi-modal learning. However, most existing methods adopt cross-modal prompt isolation, constructing visual and textual prompts separately, which exacerbates modality imbalance and leads to degraded performance over time. To tackle this issue, we propose MM-Prompt, a novel framework incorporating cross-modal prompt query and cross-modal prompt recovery. The former enables balanced prompt selection by incorporating cross-modal signals during query formation, while the latter promotes joint prompt reconstruction through iterative cross-modal interactions, guided by an alignment loss to prevent representational drift. Extensive experiments show that MM-Prompt surpasses prior approaches in accuracy and knowledge retention, while maintaining balanced modality engagement throughout continual learning.</p>
                    
                    <h2>2024</h2>
                    
                    <p><strong>Enhancing Video-Based Emotion Recognition with Multi-Head Attention and Modality Dropout</strong><br>
                    <em>Xu Li</em><br>
                    2nd International Conference on Machine Learning and Automation (CONF-MLA 2024)<br>
                    <a href="http://dx.doi.org/10.4108/eai.21-11-2024.2354608" target="_blank">paper</a></p>
                    
                    <h2>2023-2024</h2>
                    
                    <p><strong>Exploring the effect of depth and width of CNN models on binary classification of dogs and cats</strong><br>
                    <em>Xu Li</em><br>
                    Conference Proceedings<br>
                    <a href="https://api.semanticscholar.org/CorpusID:268438312" target="_blank">paper</a></p>
                </div>
            </div>
        </section>
    </main>

    <script src="minimal-script.js"></script>
</body>
</html> 